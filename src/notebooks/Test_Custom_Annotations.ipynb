{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Custom Annotations\n",
    "\n",
    "In this notebook, we illustrate how to load an annotations file from one of the *Digital Humanities Datasets* (i.e., artistical or archeological data) so as to fit them into the COCO API.\n",
    "\n",
    "In addition to the code examples, we also give an overview of the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import copy\n",
    "import json \n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "from resources_angel.utils import print_pairs\n",
    "from resources_angel.detection_coco_eval import CocoEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = np.version.version  # important to work with numpy 1.17 or below\n",
    "assert int(version.split(\".\")[1]) < 18, \"Downgrade your numpy to 1.17 or below\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "JSON_FILE_NAME = \"all_data.json\"\n",
    "JSON_PATH = os.path.join(os.getcwd(), \"resources_angel\", JSON_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Inspection\n",
    "\n",
    "In this section, we load the *JSON* file with the preprocessed (CSV to JSON) annotations and display some stats and a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading annotations\n",
    "with open(JSON_PATH) as file:\n",
    "    annotations = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = annotations[\"annotations\"]\n",
    "n_instances = len(instances)\n",
    "\n",
    "classes = [(ann[\"id\"],ann[\"name\"]) for ann in annotations[\"categories\"]]\n",
    "n_classes = len(classes)\n",
    "\n",
    "imgs = [(img[\"id\"],img[\"file_name\"]) for img in annotations[\"images\"]]\n",
    "n_imgs = len(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: \n",
      "---------\n",
      "   Total on 31 classes:\n",
      "      1:abduction  2:abductor  3:abductee  4:pursuit  5:fleeing  6:persecutor  7:lion  8:Heracles  9:wrestling (mythological)  \n",
      "      10:Nereus  11:Triton  12:Procrustes  13:Atalante  14:Peleus  15:Theseus  16:Kerkyon  17:Skiron  \n",
      "      18:Antaios  19:Eros  20:Anteros  21:Thetis  22:Acheloos  23:satyr  24:maenad  25:boar  \n",
      "      26:bull  27:Minotaur  28:cheir epi karpo  29:bride  30:groom  31:leading of a bride  \n",
      "\n",
      "Images: \n",
      "-------\n",
      "Total of 1366 images\n",
      "      0:/home/corrales/MasterThesis/data/class_arch_data/abduction/e0001.jpg  \n",
      "      1:/home/corrales/MasterThesis/data/class_arch_data/abduction/e0001-2.jpg  \n",
      "      2:/home/corrales/MasterThesis/data/class_arch_data/abduction/e0002.jpg  \n",
      "      \n",
      "\n",
      "Annotations: \n",
      "------------\n",
      "Total of 4092 person instances annotated\n",
      "   {'id': 0, 'image_id': 0, 'img_name': 'e0001.jpg', 'filename': '/home/corrales/MasterThesis/data/class_arch_data/abduction/e0001.jpg', 'bbox': '86,102,1794,2481', 'category_id': 1, 'iscrowd': 0, 'area': 4063332}\n",
      "   {'id': 1, 'image_id': 0, 'img_name': 'e0001.jpg', 'filename': '/home/corrales/MasterThesis/data/class_arch_data/abduction/e0001.jpg', 'bbox': '30,544,1626,2455', 'category_id': 2, 'iscrowd': 0, 'area': 3049956}\n",
      "   {'id': 2, 'image_id': 0, 'img_name': 'e0001.jpg', 'filename': '/home/corrales/MasterThesis/data/class_arch_data/abduction/e0001.jpg', 'bbox': '396,36,1865,2465', 'category_id': 3, 'iscrowd': 0, 'area': 3568201}\n",
      "   {'id': 3, 'image_id': 1, 'img_name': 'e0001-2.jpg', 'filename': '/home/corrales/MasterThesis/data/class_arch_data/abduction/e0001-2.jpg', 'bbox': '966,153,1626,1344', 'category_id': 4, 'iscrowd': 0, 'area': 786060}\n",
      "   {'id': 4, 'image_id': 1, 'img_name': 'e0001-2.jpg', 'filename': '/home/corrales/MasterThesis/data/class_arch_data/abduction/e0001-2.jpg', 'bbox': '1136,0,1705,1215', 'category_id': 4, 'iscrowd': 0, 'area': 691335}\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes: \")\n",
    "print(\"---------\")\n",
    "print(f\"   Total on {n_classes} classes:\")\n",
    "print_pairs(classes)\n",
    "\n",
    "print(\"\\n\\nImages: \")\n",
    "print(\"-------\")\n",
    "print(f\"Total of {n_imgs} images\")\n",
    "print_pairs(imgs[:3], n_pairs=1)\n",
    "\n",
    "print(\"\\n\\nAnnotations: \")\n",
    "print(\"------------\")\n",
    "print(f\"Total of {n_instances} person instances annotated\")\n",
    "for i in range(5):\n",
    "    print(f\"   {instances[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we need three different fields in the *annotations* json. The script **aux_process_arch_data.py** automatically extracts those from the original *.csv* annotation files:\n",
    " - **images**: Maps the unique ID of a dataset sample (*image_id*) to the name or path of the image used for loading.\n",
    " \n",
    " - **categories**: Maps the numeric identifier of a class with its semantic label (e.g., 1: 'abduction')\n",
    " \n",
    " - **annotations**: Dictionary with the detection information for each of the annotated instances in the dataset. It must containg the following fields:\n",
    "     - **id**: Unique identifier of the detection instance\n",
    "     - **image_id**: Identifier of the image to which the current instance belongs to\n",
    "     - **bbox**: coordinates of the bounding box containing the detection. Format is (x_min, y_min, x_max, y_max)\n",
    "     - **category_id**: numeric identifier of the class to which the detection corresponds\n",
    "     - **iscrowd**: If 1, detection is considered as part of a crowd and results do not count. It is hardcoded to 0 for our data.\n",
    "     - **area**: Area (in pixels) of the bounding box. COCO discards annotatios with areas too large and too small\n",
    "     - **img_name** & **filename**: These are not necessary, but I add them for completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "COCO wants the bounding boxes in the format (x_min, y_min, width, height). Therefore, we need to convert the annotations into the desired format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_instances = annotations[\"annotations\"]\n",
    "n_instances = len(processed_instances)\n",
    "\n",
    "for inst in processed_instances:\n",
    "    xmin, ymin, xmax, ymax = [int(c) for c in inst[\"bbox\"].split(\",\")]\n",
    "    coords = [xmin, ymin, xmax - xmin, ymax - ymin]  # converting to x,y,w,h\n",
    "    inst[\"bbox\"] = coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Annotations: \n",
      "------------\n",
      "Total of 4092 person instances annotated\n",
      "   {'id': 0, 'image_id': 0, 'img_name': 'e0001.jpg', 'filename': '/home/corrales/MasterThesis/data/class_arch_data/abduction/e0001.jpg', 'bbox': [86, 102, 1708, 2379], 'category_id': 1, 'iscrowd': 0, 'area': 4063332}\n",
      "   {'id': 1, 'image_id': 0, 'img_name': 'e0001.jpg', 'filename': '/home/corrales/MasterThesis/data/class_arch_data/abduction/e0001.jpg', 'bbox': [30, 544, 1596, 1911], 'category_id': 2, 'iscrowd': 0, 'area': 3049956}\n",
      "   {'id': 2, 'image_id': 0, 'img_name': 'e0001.jpg', 'filename': '/home/corrales/MasterThesis/data/class_arch_data/abduction/e0001.jpg', 'bbox': [396, 36, 1469, 2429], 'category_id': 3, 'iscrowd': 0, 'area': 3568201}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nAnnotations: \")\n",
    "print(\"------------\")\n",
    "print(f\"Total of {n_instances} person instances annotated\")\n",
    "for i in range(3):\n",
    "    print(f\"   {processed_instances[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_annotations = {\n",
    "    \"annotations\": processed_instances,\n",
    "    \"images\": annotations[\"images\"],\n",
    "    \"categories\": annotations[\"categories\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fitting COCO API\n",
    "\n",
    "We now use the loaded annotations to fit the COCO API and a COCO Evaluator for further evaluation of the detected results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# intiializing COCO dataset and fitting the annotations\n",
    "coco_dataset = COCO()\n",
    "coco_dataset.dataset = fit_annotations\n",
    "coco_dataset.createIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is necessary to let COCO know that we only do BBOX detection, but not keypoint or mask\n",
    "iou_types = [\"bbox\"]\n",
    "# intializing COCO evaluator\n",
    "coco_evaluator = CocoEvaluator(coco_dataset, iou_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simulating Evaluation\n",
    "\n",
    "To make sure everything works alright, we simulate the evaluation process. For simplicity, we use the annotations (instead of the outputs of a CNN) for the evaluation, thus obtaining 100% mAP.\n",
    "\n",
    "Somehow, we only obtain 96.8% mAP. I have not found out yet why this happens, but I conjecture it might be due to rounding issues or to some annotation that is filtered because it does not fulfill the area condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading annotations\n",
    "with open(JSON_PATH) as file:\n",
    "    annotations = json.load(file)\n",
    "\n",
    "# iterating all images\n",
    "for i, img in enumerate(annotations[\"images\"]):\n",
    "    img_id = img[\"id\"]\n",
    "    boxes, scores, labels = [], [], []\n",
    "    \n",
    "    # obtaining all annotations with the current image id\n",
    "    for j, instance in enumerate(annotations[\"annotations\"]):\n",
    "        inst_id = int(instance[\"image_id\"])\n",
    "        if(inst_id != img_id):\n",
    "            continue\n",
    "        # saving relevant features of current instance\n",
    "        boxes.append([int(coord) for coord in instance[\"bbox\"].split(\",\")])  # coords are string when reading from file\n",
    "        labels.append(instance[\"category_id\"])\n",
    "        scores.append(1)\n",
    "    \n",
    "    # the results must be given in this shape to the COCO Evaluator\n",
    "    output = {\n",
    "        \"scores\": torch.Tensor(scores),\n",
    "        \"labels\": torch.Tensor(labels),\n",
    "        \"boxes\": torch.Tensor(boxes)\n",
    "    }\n",
    "    res = {img_id: output}\n",
    "    # updating the evaluator with current results\n",
    "    coco_evaluator.update(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulating evaluation results...\n",
      "DONE (t=0.30s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.968\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.968\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.968\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.968\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.616\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.968\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.968\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.968\n"
     ]
    }
   ],
   "source": [
    "coco_evaluator.synchronize_between_processes()\n",
    "coco_evaluator.accumulate()\n",
    "valid_stats = coco_evaluator.summarize()[\"bbox\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=alert style=\"background-color:#F5F5F5; border-color:#C8C8C8\">\n",
    "   This notebook was created by <b>Angel Villar-Corrales</b>\n",
    "</div> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
