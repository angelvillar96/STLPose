{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Evaluation Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import DataParallel\n",
    "import torchvision\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from data import StyledCoco\n",
    "import models\n",
    "from CONFIG import CONFIG\n",
    "import data\n",
    "import data.custom_transforms as custom_transforms\n",
    "import data.data_processing as data_processing\n",
    "import lib.arguments as arguments\n",
    "import lib.utils as utils\n",
    "import lib.metrics as metrics\n",
    "import lib.pose_parsing as pose_parsing \n",
    "import lib.visualizations as visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_exp = \"test/experiment_2020-06-04_08-06-38\"\n",
    "test_exp = os.path.join(CONFIG[\"paths\"][\"experiments_path\"], test_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data = utils.load_experiment_parameters(test_exp)\n",
    "exp_data[\"dataset\"][\"dataset_name\"] = \"coco\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.41s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "_, valid_loader = data.load_dataset(exp_data=exp_data, train=False,\n",
    "                                    validation=True, shuffle_train=True,\n",
    "                                    shuffle_valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = models.OpenPose()\n",
    "pretrained_path = os.path.join(CONFIG[\"paths\"][\"pretrained_path\"], \"OpenPose\", \"checkpoint_iter_370000.pth\")\n",
    "model.load_pretrained(pretrained_path)\n",
    "\n",
    "model = DataParallel(model).to(device)\n",
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and Computing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 0\n",
      "Processing image 5\n",
      "Processing image 10\n",
      "Processing image 15\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    utils.reset_predictions_file(test_exp)\n",
    "    for i, (img, _, _, _, metadata) in enumerate(valid_loader):\n",
    "        if(i % 5 == 0):\n",
    "            print(f\"Processing image {i}\")\n",
    "        if(i == 15):\n",
    "            break\n",
    "        img = img.float().to(device)\n",
    "        stages_output = model(img)\n",
    "        heatmaps = stages_output[-2].cpu().detach().numpy()[0,:]\n",
    "        pafs = stages_output[-1].cpu().detach().numpy()[0,:]\n",
    "\n",
    "        # computing predictions, loading previous results and appending new ones\n",
    "        cur_predictions = metrics.compute_predictions(heatmaps, pafs, metadata)\n",
    "        utils.update_predictions_file(cur_predictions, test_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = CONFIG[\"paths\"][\"data_path\"]\n",
    "labels_path = os.path.join(data_path, \"annotations\")\n",
    "labels_file = os.path.join(labels_path, \"person_keypoints_validation.json\")\n",
    "preds_file = os.path.join(test_exp, CONFIG[\"paths\"][\"submission\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.86s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *keypoints*\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.187\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.305\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.196\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.189\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.184\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.229\n",
      " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.333\n",
      " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.250\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.187\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.300\n"
     ]
    }
   ],
   "source": [
    "stats = metrics.compute_precision(labels_file=labels_file, preds_file=preds_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
